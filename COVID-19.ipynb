{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID 19 Analysis\n",
    "a topic-modeling approch\n",
    "\n",
    "Kiarash Kiani\n",
    "\n",
    "kiani@kiarash.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from hazm import word_tokenize, stopwords_list\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import pandas as pd\n",
    "import emoji\n",
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Labeled-Data-v1.csv')\n",
    "docs = df['Content'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    return emoji.get_emoji_regexp().sub(u'', text)\n",
    "\n",
    "docs = [remove_emoji(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenizing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = [word_tokenize(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Removing Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Comparing hazm stopwords with our collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "hazm stopwords: 389, my stopwords: 1316\n"
    }
   ],
   "source": [
    "MY_STOPWORDS = []\n",
    "HAZM_STOPWORDS = stopwords_list()\n",
    "\n",
    "with open('data/stopwords.txt', encoding='utf-8') as words_file:\n",
    "    MY_STOPWORDS = words_file.read().split('\\n')\n",
    "\n",
    "print(f'hazm stopwords: {len(HAZM_STOPWORDS)}, my stopwords: {len(MY_STOPWORDS)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = [[word for word in doc if not word in MY_STOPWORDS] for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create a corpus from a list of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dictionary = Dictionary(cleaned_data)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in cleaned_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(common_corpus, num_topics=20, id2word = common_dictionary, passes = 100, alpha='auto', update_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Topic: 4, Words: [پرداخت, اپ, کرونا, واتس, عوارض, خودروسازان, شماره, #کرونا, ازادراه, اپلیکیشن]\nTopic: 8, Words: [?, کرونا, سوال, ایران, آمریکا, 95062, #کرونا, خانه, px, حروف]\nTopic: 1, Words: [?, کرونا, #کرونا, https, ایران, جزء, =, ️, ??, ویروس]\nTopic: 6, Words: [کرونا, ایران, سال, بیمه, کار, #کرونا, دانشگاه, بیماری, دنیا, خانگی]\nTopic: 5, Words: [کرونا, سال, کشور, #کرونا, بیماری, ?, RT, چین, ملت, مرحوم]\nTopic: 15, Words: [?, نفر, کشور, کووید, کرونا, بیماران, ۱۹, فروردین, مبتلا, بیماری]\nTopic: 3, Words: [نفر, کرونا, مبتلا, ویروس, تعداد, مبتلایان, استان, بیماری, کشور, ابتلا]\nTopic: 16, Words: [کرونا, ایران, سال, کشور, ویروس, دولت, تولید, شیوع, کار, اقتصادی]\nTopic: 19, Words: [?, #کرونا, #کروناویروس, ??, کرونا, #ویروس_کرونا, #ایران, https, //t, #تهران]\nTopic: 14, Words: [کرونا, استان, ویروس, بیماری, بهداشت, شیوع, بهداشتی, کشور, ستاد, مقابله]\n"
    }
   ],
   "source": [
    "for idx, topic in lda.show_topics(formatted=False, num_words= 10):\n",
    "        print('Topic: {}, Words: [{}]'.format(idx, ', '.join([w[0] for w in topic])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10\n"
    }
   ],
   "source": [
    "print(len(lda.show_topics(num_words=20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitvenvvenv2f24af0bfd77481cb4a4a887d99637ff",
   "display_name": "Python 3.6.9 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}